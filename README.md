![Imagem tem√°tica gerada com o chatgpt para ser usada como a capa do MVP](https://github.com/paulalcardoso/MVP_machine_learning/blob/main/Imagem_Capa_MVP_ML.png)

# üé¨ MVP Machine Learning & Analytics

---

## 1. Defini√ß√£o do Problema  

Este projeto foi desenvolvido como um MVP do m√≥dulo Machine Learning & Analytics para **prever qual filme ser√° o vencedor da categoria Best Picture (Melhor Filme) do Oscar**, a partir de dados hist√≥ricos dos indicados. Meu desafio foi transformar a pergunta **‚Äúqual ser√° o filme vencedor do Oscar de Melhor Filme?‚Äù** em um problema de aprendizado de m√°quina. Para isso, considerei o problema como uma **classifica√ß√£o**: em cada ano, existem v√°rios filmes indicados, mas apenas um vencedor. A minha premissa foi que **dados de bilheteria poderiam ser bons preditores**, pois podem adicionar informa√ß√µes mais relevantes do que apenas quem s√£o os atores ou quais est√∫dios produziram o filme. 

Um dos dificultadores foi trabalhar com **datasets diferentes** do Kaggle. De um lado, usei dados hist√≥ricos das nomea√ß√µes ao Oscar; de outro, dados sobre os filmes coletados no IMDb. O cruzamento dessas informa√ß√µes exigiu bastante tratamento e, nesse processo, acabei perdendo alguns filmes que n√£o identifiquei em ambas as bases.  

No fim, utilizei dois conjuntos de dados principais:  

**`full_data.csv`** ‚Äì cont√©m as informa√ß√µes de todas as nomea√ß√µes ao Oscar, em todas as categorias. Suas colunas s√£o:  
- **Ceremony** ‚Üí N√∫mero da cerim√¥nia.  
- **Year** ‚Üí Ano da premia√ß√£o.  
- **Class** ‚Üí Grande categoria da premia√ß√£o.  
- **CanonicalCategory** ‚Üí Categoria padronizada.  
- **Category** ‚Üí Categoria original.  
- **NomId** ‚Üí Identificador √∫nico da nomea√ß√£o.  
- **Film** ‚Üí Nome do filme indicado.  
- **FilmId** ‚Üí Identificador do filme.  
- **Name** ‚Üí Nome da pessoa indicada (ator, diretor, etc.), quando aplic√°vel.  
- **Nominees** ‚Üí Lista de indicados associados.  
- **NomineeIds** ‚Üí Identificadores dos indicados.  
- **Winner** ‚Üí Indica se venceu (True/False).  
- **Detail** ‚Üí Informa√ß√£o adicional sobre o papel ou contribui√ß√£o.  
- **Note** ‚Üí Observa√ß√µes adicionais.  
- **Citation** ‚Üí Cita√ß√£o associada, se houver.  
- **MultifilmNomination** ‚Üí Indica se a nomea√ß√£o est√° associada a mais de um filme.  

**`cleaned_data_from_1920_to_2025.csv`** ‚Äì Informa√ß√µes tratadas de bilheteria. Suas colunas s√£o:  
- **id** ‚Üí Identificador IMDb do filme.  
- **title** ‚Üí T√≠tulo do filme.  
- **duration** ‚Üí Dura√ß√£o.  
- **MPA** ‚Üí Classifica√ß√£o indicativa.  
- **rating** ‚Üí Nota m√©dia no IMDb.  
- **votes** ‚Üí N√∫mero de votos no IMDb.  
- **meta_score** ‚Üí Avalia√ß√£o de cr√≠ticos.  
- **description** ‚Üí Sinopse do filme.  
- **movie_link** ‚Üí Link IMDb.  
- **writers** ‚Üí Lista de roteiristas.  
- **directors** ‚Üí Lista de diretores.  
- **stars** ‚Üí Atores principais.  
- **budget** ‚Üí Or√ßamento estimado.  
- **opening_weekend_gross** ‚Üí Receita de estreia.  
- **gross_worldwide** ‚Üí Bilheteria mundial.  
- **gross_us_canada** ‚Üí Bilheteria nos EUA e Canad√°.  
- **release_date** ‚Üí Data de lan√ßamento.  
- **countries_origin** ‚Üí Pa√≠s(es) de origem.  
- **filming_locations** ‚Üí Locais de filmagem.  
- **production_companies** ‚Üí Est√∫dios/produtoras.  
- **awards_content** ‚Üí Resumo de pr√™mios e indica√ß√µes.  
- **genres** ‚Üí G√™neros.  
- **languages** ‚Üí Idiomas.  

---

## 2. Prepara√ß√£o de Dados  

Antes de separar os dados para treino e teste, precisei realizar diversos ajustes, como corrigir o ano de lan√ßamento e normalizar o t√≠tulo dos filmes. Utilizei uma chave composta de **t√≠tulo + ano de lan√ßamento** para unir os datasets e defini uma **regra de toler√¢ncia de at√© 3 anos** (baseada em estat√≠sticas) para aumentar a quantidade de filmes corretamente combinados.  

Ap√≥s a jun√ß√£o, tratei **linhas duplicadas**, mantendo apenas a mais completa (com menos valores nulos), e utilizei **fuzzy matching** para identificar pares de t√≠tulos que ainda n√£o haviam sido associados. Com esse processo, consegui identificar **608 dos 611 filmes indicados**.  

Em seguida, reduzi a dimens√£o do dataset eliminando vari√°veis irrelevantes (como links para p√°ginas do IMDb), ficando com **9 atributos** a serem usados na predi√ß√£o. Neste MVP  a vari√°vel target √© a coluna **Winner**, com valor **True** para vencedores e **nulo** para demais indicados.  

Na sequ√™ncia, dividi os dados em **treino (80%) e teste (20%) de forma estratificada**, garantindo que a propor√ß√£o entre as classes fosse preservada. IA valida√ß√£o cruzada estratificada (k=5) ser√° aplicada apenas no modelo candidato final.  

Al√©m disso, apliquei uma s√©rie de **transforma√ß√µes de dados**:  
- **Converter** colunas financeiras de `object` para `float`, desconsiderando valores que n√£o estavam em d√≥lar.  
- **Criar** colunas auxiliares (`_missing`) para sinalizar valores ausentes antes do tratamento.  
- **Aplicar** One-Hot Encoding em vari√°veis categ√≥ricas (pa√≠ses, g√™neros etc.).  
- **Criar** coluna com a quantidade de indica√ß√µes do filme em outras categorias.  
- **Normalizar** vari√°veis num√©ricas, gerando vers√µes escaladas para compara√ß√£o.  

Para avaliar o impacto dessas transforma√ß√µes, criei **quatro pipelines de pr√©-processamento**:  
1. sem normaliza√ß√£o e sem feature selection;  
2. sem normaliza√ß√£o e com feature selection;  
3. com normaliza√ß√£o e sem feature selection;  
4. com normaliza√ß√£o e com feature selection.  

---

## 3. Modelagem e Treinamento  

Para estabelecer uma refer√™ncia m√≠nima de desempenho, utilizei como baseline o **DummyClassifier** (*most_frequent*). Esse modelo sempre prev√™ a classe majorit√°ria (n√£o vencedor) e, portanto, apresenta recall nulo para vencedores.  

Al√©m do baseline, avaliei dois modelos supervisionados:  
- **Logistic Regression**, configurada com `class_weight="balanced"`, atribuindo maior peso para a classe minorit√°ria (vencedores).  
- **XGBoost**, com o par√¢metro `scale_pos_weight`, definido como a raz√£o entre inst√¢ncias negativas (indicados) e positivas (vencedores).  

Ambos os modelos foram treinados nos quatro pipelines de pr√©-processamento, criando quatro vis√µes diferentes. Tanto a Regress√£o Log√≠stica quanto o XGBoost superaram o baseline. A Regress√£o Log√≠stica obteve **recall mais alto**, identificando mais vencedores, mas √† custa de menor precis√£o e F1. J√° o XGBoost apresentou **desempenho mais consistente em termos de F1 ponderado**, mesmo com recall menor, al√©m de ser robusto a diferentes transforma√ß√µes.  

Na etapa seguinte, apliquei **valida√ß√£o cruzada estratificada (StratifiedKFold)** junto com o **RandomizedSearchCV** para otimizar o XGBoost. Inicialmente observei **overfitting**, j√° que as m√©tricas no treino estavam muito acima das m√©tricas de valida√ß√£o. Para mitigar, aumentei a lista de hiperpar√¢metros: al√©m de `n_estimators`, `max_depth` e `learning_rate`, inclui `min_child_weight`, `subsample` e `colsample_bytree`, al√©m de regulariza√ß√µes. O objetivo foi equilibrar a complexidade do modelo, reduzindo falsos positivos e priorizando a identifica√ß√£o da classe vencedora.  

---

## 4. Avalia√ß√£o de Resultados  

Como o dataset √© fortemente desbalanceado, a **acur√°cia n√£o √© uma m√©trica adequada**: um modelo que sempre prev√™ a classe majorit√°ria j√° alcan√ßaria acur√°cia alta sem resolver o problema. Por isso, concentrei a avalia√ß√£o em **Precis√£o, Recall e F1-Score da classe vencedora**, al√©m do **F1 ponderado** para medir o equil√≠brio global.  

Com a otimiza√ß√£o, o XGBoost foi **re-treinado em toda a base de treino** (autom√°tico pelo `RandomizedSearchCV`) e s√≥ depois avaliado no conjunto de teste. O trade-off observado foi claro: houve **pequena perda em recall**, mas em troca os resultados de valida√ß√£o ficaram mais pr√≥ximos dos de treino, indicando que o modelo passou a **generalizar melhor**, apesar de ainda apresentar overfit.  

Tamb√©m testei um **ensemble heterog√™neo (Voting Classifier)** combinando o XGBoost otimizado e a Regress√£o Log√≠stica balanceada. Embora tenha aumentado o recall, perdeu em precis√£o e n√£o superou o XGBoost otimizado em termos de F1 global.  

#### Compara√ß√£o das M√©tricas  

| Modelo               | Precision (classe vencedora) | Recall (classe vencedora) | F1 (classe vencedora) | F1 Ponderado |
|----------------------|------------------------------|----------------------------|------------------------|--------------|
| **DummyClassifier**  | 0.00                         | 0.00                       | 0.00                   | 0.77         |
| **XGBoost otimizado**| 0.33                         | 0.32                       | 0.32                   | 0.79         |
| **Ensemble (Voting)**| 0.26                         | 0.37                       | 0.30                   | 0.76         | 

---
## 5. Conclus√µes e Pr√≥ximos Passos  

### Conclus√µes  
- O problema de previs√£o dos vencedores do Oscar foi tratado como uma **classifica√ß√£o** em dados tabulares, com forte desbalanceamento entre indicados (classe majorit√°ria) e vencedores (classe minorit√°ria).  
- O **Dummy Classifier** serviu como baseline, confirmando que qualquer modelo √∫til precisaria superar o simples acerto da classe majorit√°ria.  
- Entre os modelos testados, o **XGBoost** apresentou desempenho mais consistente que a Regress√£o Log√≠stica, especialmente no equil√≠brio entre *recall* e *precision*.  
- O **Voting Ensemble** (heterog√™neo, combinando Regress√£o Log√≠stica e XGBoost) foi avaliado, mas n√£o trouxe ganhos em rela√ß√£o ao XGBoost otimizado, refor√ßando a escolha deste como modelo final.  
- O ajuste de hiperpar√¢metros via *RandomizedSearchCV* melhorou o desempenho do XGBoost, aumentando sua capacidade de generaliza√ß√£o, mas aumentou significativamente o tempo de treinamento (vide notebook).  
- O trabalho refor√ßou a import√¢ncia de m√©tricas como *recall* e *F1-score* para a classe de vencedores, uma vez que a simples acur√°cia √© enganosa no contexto de classes desbalanceadas.
- O desafio do MVP foi encontrar o ponto de equil√≠brio entre *aumentar recall e gerar mais falsos positivos*.

### Trade-offs observados  
- O aumento do *recall* veio em detrimento da *precision*: ao identificar mais vencedores, o modelo tamb√©m trouxe mais falsos positivos.  
- O uso de ensemble ampliou a cobertura (maior *recall*), mas reduziu a confian√ßa nas previs√µes (queda na *precision* e no *F1*).  

### Pr√≥ximos passos  
- **Dados**: incorporar mais atributos dos filmes (ex.: indica√ß√µes em outras categorias do Oscar, bilheteria detalhada) para enriquecer a base. Tamb√©m considerar prever vencedores como um todo das premia√ß√µes ao Oscar, e n√£o s√≥ de *Best Picture*.  
- **Modelos**: testar m√©todos mais avan√ßados de balanceamento de classes (ex.: SMOTE, ajustes adicionais em *class weights*).  
- **Tuning**: experimentar *Bayesian Optimization* ou outros m√©todos mais eficientes de busca para otimiza√ß√£o dos hiperpar√¢metros, reduzindo o custo de tempo de treinamento.  
