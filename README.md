![Imagem tem√°tica gerada com o chatgpt para ser usada como a capa do MVP](https://github.com/paulalcardoso/MVP_machine_learning/blob/main/Imagem_Capa_MVP_ML.png)

# MVP Machine Learning & Analytics
Este projeto foi desenvolvido como um MVP do m√≥dulo Machine Learning & Analytics para **prever qual filme ser√° o vencedor da categoria Best Picture (Melhor Filme) do Oscar**, a partir de dados hist√≥ricos dos indicados.

Meu desafio foi transformar a pergunta ‚Äúqual ser√° o filme vencedor do Oscar de Melhor Filme?‚Äù em um problema de aprendizado de m√°quina. Para isso, considerei o problema como uma classifica√ß√£o: em cada ano, existem v√°rios filmes indicados, mas apenas um vencedor. A minha premissa foi que dados de bilheteria poderiam ser bons preditores, pois podem adicionar informa√ß√µes mais relevantes do que quem s√£o os atores ou quais est√∫dios produziram o filme. 

Um dos dificultadores foi trabalhar com datasets diferentes do Kaggle. De um lado, usei dados hist√≥ricos das nomea√ß√µes ao Oscar; de outro, dados sobre os filmes coletados no IMDb. O cruzamento dessas informa√ß√µes exigiu bastante tratamento e, nesse processo, acabei perdendo alguns filmes, que n√£o identifiquei em ambas as bases.

No fim, utilizei dois conjuntos de dados principais:

**full_data.csv** ‚Äì cont√©m as informa√ß√µes de todas as nomea√ß√µes ao Oscar, em todas as categorias. Suas colunas s√£o:
**Ceremony** ‚Üí N√∫mero da cerim√¥nia.
**Year** ‚Üí Ano da premia√ß√£o.
**Class** ‚Üí Grande categoria da premia√ß√£o.
**CanonicalCategory** ‚Üí Categoria padronizada.
**Category** ‚Üí Categoria original.
**NomId** ‚Üí Identificador √∫nico da nomea√ß√£o.
**Film** ‚Üí Nome do filme indicado.
**FilmId** ‚Üí Identificador do filme.
**Name** ‚Üí Nome da pessoa indicada (ator, diretor, etc.), quando aplic√°vel.
**Nominees** ‚Üí Lista de indicados associados.
**NomineeIds** ‚Üí Identificadores dos indicados.
**Winner** ‚Üí Indica se venceu (True/False).
**Detail** ‚Üí Informa√ß√£o adicional sobre o papel ou contribui√ß√£o.
**Note** ‚Üí Observa√ß√µes adicionais.
**Citation** ‚Üí Cita√ß√£o associada, se houver.
**MultifilmNomination** ‚Üí Indica se a nomea√ß√£o est√° associada a mais de um filme.

**cleaned_data_from_1920_to_2025.csv** ‚Äì Informa√ß√µes tratadas de bilheteria. Suas colunas s√£o:
**id** ‚Üí Identificador IMDb do filme.
**title** ‚Üí T√≠tulo do filme.
**duration** ‚Üí Dura√ß√£o.
**MPA** ‚Üí Classifica√ß√£o indicativa.
**rating** ‚Üí Nota m√©dia no IMDb.
**votes** ‚Üí N√∫mero de votos no IMDb.
**meta_score** ‚Üí Avalia√ß√£o de cr√≠ticos.
**description** ‚Üí Sinopse do filme.
**movie_link** ‚Üí Link IMDb.
**writers** ‚Üí Lista de roteiristas.
**directors** ‚Üí Lista de diretores.
**stars** ‚Üí Atores principais.
**budget** ‚Üí Or√ßamento estimado.
**opening_weekend_gross** ‚Üí Receita de estreia.
**gross_worldwide** ‚Üí Bilheteria mundial.
**gross_us_canada** ‚Üí Bilheteria nos EUA e Canad√°.
**release_date** ‚Üí Data de lan√ßamento.
**countries_origin** ‚Üí Pa√≠s(es) de origem.
**filming_locations** ‚Üí Locais de filmagem.
**production_companies** ‚Üí Est√∫dios/produtoras.
**awards_content** ‚Üí Resumo de pr√™mios e indica√ß√µes.
**genres** ‚Üí G√™neros.
**languages** ‚Üí Idiomas.

Antes de separar os dados para treino e teste, precisei realizar diversos ajustes, como corrigir o ano de lan√ßamento e normalizar o t√≠tulo dos filmes. Utilizei uma chave composta de t√≠tulo-ano de lan√ßamento para unir os datasets e defini uma regra de toler√¢ncia de at√© 3 anos (baseada em estat√≠sticas) para aumentar a quantidade de filmes corretamente combinados. Ap√≥s a jun√ß√£o, tratei linhas duplicadas mantendo apenas a linha mais completa (com menos valores nulos) e utilizei uma t√©cnica de fuzzy matching para identificar pares de t√≠tulos que ainda n√£o haviam sido associados. Com esse processo, consegui identificar 608 dos 611 filmes indicados. 

Uma vez com o dataset unificado, reduzi a sua dimens√£o eliminando vari√°veis irrelevantes para a predi√ß√£o, como links para p√°ginas do IMDb, ficando com 9 atributos a serem usados na predi√ß√£o. Este MVP √© um problema de classifica√ß√£o bin√°ria, em que a vari√°vel target √© a coluna Winner, oriunda do dataset dos filmes indicados, com valor True para os vencedores e nulo para os demais indicados.

Na sequ√™ncia, dividi os dados em treino (80%) e teste (20%) de forma estratificada, garantindo que a propor√ß√£o entre as classes fosse preservada. Inicialmente identifiquei que faria sentido utilizar valida√ß√£o cruzada estratificada (k=5), j√° que o dataset √© desbalanceado, mas optei por s√≥ aplicar essa t√©cnica no modelo selecionado como candidato final.

Al√©m disso, apliquei uma s√©rie de transforma√ß√µes de dados para tornar o conjunto mais consistente, informativo e pr√©-processado para o treinamento e teste:

- Converti colunas financeiras que estavam em formato object, convertendo-as para valores num√©ricos (float), e desconsiderei valores que n√£o s√£o em d√≥lar.

- Criei colunas auxiliares (_missing) para sinalizar valores ausentes antes de trat√°-los, de forma a n√£o perder e confundir informa√ß√£o.

- Apliquei One-Hot Encoding para transformar vari√°veis categ√≥ricas, como pa√≠ses e g√™neros, em vari√°veis num√©ricas.

- Criei uma coluna com a quantidade de indica√ß√µes do filme, como um poss√≠vel indicador de relev√¢ncia na premia√ß√£o.

- Testei a normaliza√ß√£o das vari√°veis num√©ricas, gerando vers√µes escaladas para que atributos em diferentes ordens de grandeza n√£o dominassem o aprendizado.

Para avaliar o impacto dessas transforma√ß√µes, criei diferentes vis√µes do dataset em 4 pipelines de pr√©-processamento: 

1. sem normaliza√ß√£o e sem feature selection;
   
2. sem normaliza√ß√£o e com feature selection;
   
3. com normaliza√ß√£o e sem feature selection;
   
4. com normaliza√ß√£o e com feature selection.

Esse processo me permitiu analisar como cada transforma√ß√£o influenciava os resultados e chegar a um conjunto enxuto e consistente de atributos para alimentar os modelos.

Para estabelecer uma refer√™ncia m√≠nima de desempenho, utilizei como baseline o DummyClassifier, configurado com a estrat√©gia most_frequent. Al√©m dele, avaliei dois modelos supervisionados bastante utilizados em problemas de classifica√ß√£o. O primeiro foi a Logistic Regression, configurando class_weight="balanced", que ajusta o peso de cada classe de acordo com a sua frequ√™ncia ‚Äî atribuindo maior peso para a classe minorit√°ria (vencedores) e menor para a classe majorit√°ria (demais indicados). O segundo modelo foi o XGBoost (algoritmo baseado em gradient boosting com boa capacidade de lidar com desbalanceamento), com o par√¢metro scale_pos_weight, definido como a raz√£o entre a quantidade de inst√¢ncias negativas (indicados) e positivas (vencedores) no dataset.

Ambos os modelos de regress√£o log√≠stica e XGBoost foram devidamente treinados nos quatro diferentes pipelines de pr√©-processamento apresentados anteriormente. Na compara√ß√£o entre eles, tanto a regress√£o log√≠stica quanto o XGBoost superaram o baseline do DummyClassifier. A regress√£o log√≠stica se destacou por ser simples e r√°pida, al√©m de ter obtido um recall mais alto, conseguindo identificar mais vencedores. No entanto, essa vantagem veio acompanhada de uma queda na precis√£o, o que reduziu o equil√≠brio geral e resultou em um F1 inferior. O XGBoost, por sua vez, apresentou um desempenho mais consistente em termos de F1 ponderado, mesmo com recall menor. Al√©m disso, atingiu bons resultados sem depender de normaliza√ß√£o ou sele√ß√£o de atributos, o que refor√ßa sua robustez. Esses fatores levaram √† escolha do XGBoost otimizado como modelo a ter seus hiperpar√¢metros otimizados e resultados de treino e teste avaliados.

Na etapa seguinte, ao aplicar a valida√ß√£o cruzada estratificada (StratifiedKFold) junto com o RandomizedSearchCV, percebi que o XGBoost (escolhido na etapa anterior) apresentou overfitting, devido a diferen√ßa entre os resultados de treino e valida√ß√£o. Para mitigar esse problema, ampliei o conjunto de hiperpar√¢metros a serem otimizados. Al√©m de ajustar o n√∫mero de estimadores (n_estimators), a profundidade m√°xima das √°rvores (max_depth) e a taxa de aprendizado (learning_rate), passei a incluir tamb√©m par√¢metros como min_child_weight, subsample e colsample_bytree. O objetivo foi encontrar um equil√≠brio na complexidade do modelo, buscando reduzir falsos positivos e, ao mesmo tempo, priorizar o acerto da classe dos vencedores, que √© a mais dif√≠cil de identificar nesse cen√°rio desbalanceado.

Como o meu dataset √© fortemente desbalanceado (apenas um vencedor por ano contra v√°rios indicados), a acur√°cia n√£o √© uma m√©trica adequada: mesmo um modelo que sempre prev√™ a classe majorit√°ria (n√£o vencedor) alcan√ßa valores altos de acur√°cia, sem de fato resolver o problema. Por isso, para avaliar se a otimiza√ß√£o do XGBoost foi eficaz, considerei principalmente as m√©tricas Precis√£o, Recall e F1-Score da classe vencedora, al√©m do F1 ponderado para o equil√≠brio geral do modelo. Com a amplia√ß√£o dos hiperpar√¢metros, o XGBoost foi retreinado comto o conjunto de treinoe o resultado foi um modelo mais equilibrado, mas e o trade-off ficou claro: ao buscar reduzir o overfitting e evitar tantos falsos positivos, obtive uma perda em recall, mas os resultados de valida√ß√£o ficaram mais pr√≥ximos dos obtidos no treino, mostrando que o modelo passou a generalizar melhor.

Para finalizar, testei tamb√©m um ensemble homog√™neo (Voting Classifier) combinando o XGBoost otimizado com a regress√£o log√≠stica balanceada escolhida anteriormente. A inten√ß√£o foi para reduzir falsos positivos sem comprometer o recall. No entanto, apesar de trazer algum ganho localizado em precis√£o, o ensemble n√£o superou o desempenho do XGBoost otimizado em termos de equil√≠brio geral das m√©tricas. Com isso, conclu√≠ que o XGBoost otimizado permaneceu como o melhor candidato, oferecendo o trade-off mais adequado entre precis√£o e recall e apresentando maior capacidade de generaliza√ß√£o em compara√ß√£o √†s demais abordagens.


# üé¨ MVP Machine Learning & Analytics

Este projeto foi desenvolvido como um MVP do m√≥dulo Machine Learning & Analytics para **prever qual filme ser√° o vencedor da categoria Best Picture (Melhor Filme) do Oscar**, a partir de dados hist√≥ricos dos indicados.

---

## 1. Defini√ß√£o do Problema  

Meu desafio foi transformar a pergunta **‚Äúqual ser√° o filme vencedor do Oscar de Melhor Filme?‚Äù** em um problema de aprendizado de m√°quina. Para isso, considerei o problema como uma **classifica√ß√£o**: em cada ano, existem v√°rios filmes indicados, mas apenas um vencedor.  

A minha premissa foi que **dados de bilheteria poderiam ser bons preditores**, pois podem adicionar informa√ß√µes mais relevantes do que apenas quem s√£o os atores ou quais est√∫dios produziram o filme.  

Um dos dificultadores foi trabalhar com **datasets diferentes** do Kaggle. De um lado, usei dados hist√≥ricos das nomea√ß√µes ao Oscar; de outro, dados sobre os filmes coletados no IMDb. O cruzamento dessas informa√ß√µes exigiu bastante tratamento e, nesse processo, acabei perdendo alguns filmes que n√£o identifiquei em ambas as bases.  

No fim, utilizei dois conjuntos de dados principais:  

**`full_data.csv`** ‚Äì cont√©m as informa√ß√µes de todas as nomea√ß√µes ao Oscar, em todas as categorias. Suas colunas s√£o:  
- **Ceremony** ‚Üí N√∫mero da cerim√¥nia.  
- **Year** ‚Üí Ano da premia√ß√£o.  
- **Class** ‚Üí Grande categoria da premia√ß√£o.  
- **CanonicalCategory** ‚Üí Categoria padronizada.  
- **Category** ‚Üí Categoria original.  
- **NomId** ‚Üí Identificador √∫nico da nomea√ß√£o.  
- **Film** ‚Üí Nome do filme indicado.  
- **FilmId** ‚Üí Identificador do filme.  
- **Name** ‚Üí Nome da pessoa indicada (ator, diretor, etc.), quando aplic√°vel.  
- **Nominees** ‚Üí Lista de indicados associados.  
- **NomineeIds** ‚Üí Identificadores dos indicados.  
- **Winner** ‚Üí Indica se venceu (True/False).  
- **Detail** ‚Üí Informa√ß√£o adicional sobre o papel ou contribui√ß√£o.  
- **Note** ‚Üí Observa√ß√µes adicionais.  
- **Citation** ‚Üí Cita√ß√£o associada, se houver.  
- **MultifilmNomination** ‚Üí Indica se a nomea√ß√£o est√° associada a mais de um filme.  

**`cleaned_data_from_1920_to_2025.csv`** ‚Äì Informa√ß√µes tratadas de bilheteria. Suas colunas s√£o:  
- **id** ‚Üí Identificador IMDb do filme.  
- **title** ‚Üí T√≠tulo do filme.  
- **duration** ‚Üí Dura√ß√£o.  
- **MPA** ‚Üí Classifica√ß√£o indicativa.  
- **rating** ‚Üí Nota m√©dia no IMDb.  
- **votes** ‚Üí N√∫mero de votos no IMDb.  
- **meta_score** ‚Üí Avalia√ß√£o de cr√≠ticos.  
- **description** ‚Üí Sinopse do filme.  
- **movie_link** ‚Üí Link IMDb.  
- **writers** ‚Üí Lista de roteiristas.  
- **directors** ‚Üí Lista de diretores.  
- **stars** ‚Üí Atores principais.  
- **budget** ‚Üí Or√ßamento estimado.  
- **opening_weekend_gross** ‚Üí Receita de estreia.  
- **gross_worldwide** ‚Üí Bilheteria mundial.  
- **gross_us_canada** ‚Üí Bilheteria nos EUA e Canad√°.  
- **release_date** ‚Üí Data de lan√ßamento.  
- **countries_origin** ‚Üí Pa√≠s(es) de origem.  
- **filming_locations** ‚Üí Locais de filmagem.  
- **production_companies** ‚Üí Est√∫dios/produtoras.  
- **awards_content** ‚Üí Resumo de pr√™mios e indica√ß√µes.  
- **genres** ‚Üí G√™neros.  
- **languages** ‚Üí Idiomas.  

---

## 2. Prepara√ß√£o de Dados  

Antes de separar os dados para treino e teste, precisei realizar diversos ajustes, como corrigir o ano de lan√ßamento e normalizar o t√≠tulo dos filmes. Utilizei uma chave composta de **t√≠tulo + ano de lan√ßamento** para unir os datasets e defini uma **regra de toler√¢ncia de at√© 3 anos** (baseada em estat√≠sticas) para aumentar a quantidade de filmes corretamente combinados.  

Ap√≥s a jun√ß√£o, tratei **linhas duplicadas**, mantendo apenas a mais completa (com menos valores nulos), e utilizei **fuzzy matching** para identificar pares de t√≠tulos que ainda n√£o haviam sido associados. Com esse processo, consegui identificar **608 dos 611 filmes indicados**.  

Em seguida, reduzi a dimens√£o do dataset eliminando vari√°veis irrelevantes (como links para p√°ginas do IMDb), ficando com **9 atributos** a serem usados na predi√ß√£o. Este MVP √©, portanto, um problema de **classifica√ß√£o bin√°ria**, em que a vari√°vel target √© a coluna **Winner**, com valor **True** para vencedores e **nulo** para demais indicados.  

Na sequ√™ncia, dividi os dados em **treino (80%) e teste (20%) de forma estratificada**, garantindo que a propor√ß√£o entre as classes fosse preservada. Inicialmente considerei utilizar valida√ß√£o cruzada estratificada (k=5) j√° nesta etapa, mas optei por aplicar essa t√©cnica apenas no modelo candidato final.  

Al√©m disso, apliquei uma s√©rie de **transforma√ß√µes de dados**:  
- **Converter** colunas financeiras de `object` para `float`, desconsiderando valores que n√£o estavam em d√≥lar.  
- **Criar** colunas auxiliares (`_missing`) para sinalizar valores ausentes antes do tratamento.  
- **Aplicar** One-Hot Encoding em vari√°veis categ√≥ricas (pa√≠ses, g√™neros etc.).  
- **Criar** coluna com a quantidade de indica√ß√µes do filme em outras categorias.  
- **Normalizar** vari√°veis num√©ricas, gerando vers√µes escaladas para compara√ß√£o.  

Para avaliar o impacto dessas transforma√ß√µes, criei **quatro pipelines de pr√©-processamento**:  
1. sem normaliza√ß√£o e sem feature selection;  
2. sem normaliza√ß√£o e com feature selection;  
3. com normaliza√ß√£o e sem feature selection;  
4. com normaliza√ß√£o e com feature selection.  

---

## 3. Modelagem e Treinamento  

Para estabelecer uma refer√™ncia m√≠nima de desempenho, utilizei como baseline o **DummyClassifier** (*most_frequent*). Esse modelo sempre prev√™ a classe majorit√°ria (n√£o vencedor) e, portanto, apresenta recall nulo para vencedores.  

Al√©m do baseline, avaliei dois modelos supervisionados:  
- **Logistic Regression**, configurada com `class_weight="balanced"`, atribuindo maior peso para a classe minorit√°ria (vencedores).  
- **XGBoost**, com o par√¢metro `scale_pos_weight`, definido como a raz√£o entre inst√¢ncias negativas (indicados) e positivas (vencedores).  

Ambos os modelos foram treinados nos quatro pipelines de pr√©-processamento. Tanto a Regress√£o Log√≠stica quanto o XGBoost superaram o baseline. A Regress√£o Log√≠stica obteve **recall mais alto**, identificando mais vencedores, mas √† custa de menor precis√£o e F1. J√° o XGBoost apresentou **desempenho mais consistente em termos de F1 ponderado**, mesmo com recall menor, al√©m de ser robusto a diferentes transforma√ß√µes.  

Na etapa seguinte, apliquei **valida√ß√£o cruzada estratificada (StratifiedKFold)** junto com o **RandomizedSearchCV** para otimizar o XGBoost. Inicialmente observei **overfitting**, j√° que as m√©tricas no treino estavam muito acima das m√©tricas de valida√ß√£o. Para mitigar, ampliei o espa√ßo de hiperpar√¢metros: al√©m de `n_estimators`, `max_depth` e `learning_rate`, inclui `min_child_weight`, `subsample` e `colsample_bytree`, al√©m de regulariza√ß√µes. O objetivo foi equilibrar a complexidade do modelo, reduzindo falsos positivos e priorizando a identifica√ß√£o da classe vencedora.  

---

## 4. Avalia√ß√£o de Resultados  

Como o dataset √© fortemente desbalanceado, a **acur√°cia n√£o √© uma m√©trica adequada**: um modelo que sempre prev√™ a classe majorit√°ria j√° alcan√ßaria acur√°cia alta sem resolver o problema. Por isso, concentrei a avalia√ß√£o em **Precis√£o, Recall e F1-Score da classe vencedora**, al√©m do **F1 ponderado** para medir o equil√≠brio global.  

Com a otimiza√ß√£o, o XGBoost foi **re-treinado em toda a base de treino** (autom√°tico pelo `RandomizedSearchCV`) e s√≥ depois avaliado no conjunto de teste. O trade-off observado foi claro: houve **pequena perda em recall**, mas em troca os resultados de valida√ß√£o ficaram mais pr√≥ximos dos de treino, indicando que o modelo passou a **generalizar melhor**.  

Tamb√©m testei um **ensemble homog√™neo (Voting Classifier)** combinando o XGBoost otimizado e a Regress√£o Log√≠stica balanceada. Embora tenha aumentado o recall, perdeu em precis√£o e n√£o superou o XGBoost otimizado em termos de F1 global.  

#### Compara√ß√£o das M√©tricas  

| Modelo               | Precision (classe vencedora) | Recall (classe vencedora) | F1 (classe vencedora) | F1 Ponderado |
|----------------------|------------------------------|----------------------------|------------------------|--------------|
| **DummyClassifier**  | 0.00                         | 0.00                       | 0.00                   | 0.77         |
| **XGBoost otimizado**| 0.33                         | 0.32                       | 0.32                   | 0.79         |
| **Ensemble (Voting)**| 0.26                         | 0.37                       | 0.30                   | 0.76         |

üìå **Interpreta√ß√£o:**  
- O Dummy confirmou a dificuldade do problema, servindo apenas como baseline.  
- O XGBoost otimizado apresentou o melhor equil√≠brio entre precis√£o, recall e F1, superando amplamente o baseline.  
- O Voting aumentou recall, mas perdeu em precis√£o e n√£o superou o XGBoost no equil√≠brio geral.  

Esses resultados fazem sentido diante da forte despropor√ß√£o entre vencedores e n√£o vencedores: **aumentar recall implica arriscar mais falsos positivos**, e o desafio √© justamente encontrar o ponto de equil√≠brio.  

Assim, conclu√≠ que o **XGBoost otimizado** √© a melhor solu√ß√£o encontrada, por apresentar o trade-off mais adequado entre precis√£o e recall e maior capacidade de generaliza√ß√£o.  

---
